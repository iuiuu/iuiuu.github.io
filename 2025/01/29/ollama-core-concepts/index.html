<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Ollama Core Concepts | Johnson Lin</title><meta name="author" content="Johnson Lin"><meta name="copyright" content="Johnson Lin"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Ollama is a localized machine learning framework designed for various natural language processing (NLP) tasks. It focuses on model loading, inference, and generation, making it easy for users to inter">
<meta property="og:type" content="article">
<meta property="og:title" content="Ollama Core Concepts">
<meta property="og:url" content="http://linjiangxiong.com/2025/01/29/ollama-core-concepts/index.html">
<meta property="og:site_name" content="Johnson Lin">
<meta property="og:description" content="Ollama is a localized machine learning framework designed for various natural language processing (NLP) tasks. It focuses on model loading, inference, and generation, making it easy for users to inter">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://linjiangxiong.com/image/IMG_3665.JPG">
<meta property="article:published_time" content="2025-01-29T04:11:16.000Z">
<meta property="article:modified_time" content="2025-02-09T17:36:01.690Z">
<meta property="article:author" content="Johnson Lin">
<meta property="article:tag" content="Ollama">
<meta property="article:tag" content="LLM">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://linjiangxiong.com/image/IMG_3665.JPG"><link rel="shortcut icon" href="/image/IMG_3665.JPG"><link rel="canonical" href="http://linjiangxiong.com/2025/01/29/ollama-core-concepts/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Nunito:wght@400;700&amp;family=Source+Sans+3:wght@400;600&amp;display=swap"><link rel="stylesheet" href="/css/ud_v5.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?ca45da0012a3ce293c6ca4f7e5ebc3a8";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
btf.addGlobalFn('pjaxComplete', () => {
  _hmt.push(['_trackPageview',window.location.pathname])
}, 'baidu_analytics')
</script><script async="async" src="https://www.googletagmanager.com/gtag/js?id=WLKqFH01ST8bHfxLtjsterJZnoEpVlF26sn-Nzzoqfc"></script><script>window.dataLayer = window.dataLayer || []
function gtag(){dataLayer.push(arguments)}
gtag('js', new Date())
gtag('config', 'WLKqFH01ST8bHfxLtjsterJZnoEpVlF26sn-Nzzoqfc')
btf.addGlobalFn('pjaxComplete', () => {
  gtag('config', 'WLKqFH01ST8bHfxLtjsterJZnoEpVlF26sn-Nzzoqfc', {'page_path': window.location.pathname})
}, 'google_analytics')
</script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Failed',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Ollama Core Concepts',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  isShuoshuo: false
}</script><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.2.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/image/IMG_3665.JPG" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">488</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">63</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">26</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> Instant Tutorials</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/2025/01/29/ollama-tutorial/"><i class="fa-fw fas fa-book"></i><span> Ollama Tutorial</span></a></li><li><a class="site-page child" href="/2024/06/29/html-instant-tutorial/"><i class="fa-fw fas fa-book"></i><span> HTML Tutorial</span></a></li><li><a class="site-page child" href="/2024/08/27/yaml-tutorial/"><i class="fa-fw fas fa-book"></i><span> YAML Tutorial</span></a></li><li><a class="site-page child" href="/categories/Bash-Tutorial/"><i class="fa-fw fas fa-book"></i><span> Bash Tutorial</span></a></li><li><a class="site-page child" href="/2023/09/05/tutorial-gson/"><i class="fa-fw fas fa-music"></i><span> 极简教程 - Gson</span></a></li><li><a class="site-page child" href="/categories/Redis%E6%95%99%E7%A8%8B/"><i class="fa-fw fas fa-book"></i><span> Redis教程</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><span> Dev Wiki</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/2024/11/29/wiki/standard-ascii-table/"><i class="fa-fw fas fa-book"></i><span> Standard ASCII Table (7-bit)</span></a></li><li><a class="site-page child" href="/2025/01/17/a-complete-guide-to-base64-encoding-and-decoding/"><i class="fa-fw fas fa-book"></i><span> Base64 Introduction</span></a></li><li><a class="site-page child" href="/2024/11/30/wiki/html-char/"><i class="fa-fw fas fa-book"></i><span> HTML Special Char</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><span> Dev Tools</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/tools/json-formatter/"><i class="fa-fw fas fa-link"></i><span> JSON Format</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">Johnson Lin</span></a><a class="nav-page-title" href="/"><span class="site-name">Ollama Core Concepts</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> Instant Tutorials</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/2025/01/29/ollama-tutorial/"><i class="fa-fw fas fa-book"></i><span> Ollama Tutorial</span></a></li><li><a class="site-page child" href="/2024/06/29/html-instant-tutorial/"><i class="fa-fw fas fa-book"></i><span> HTML Tutorial</span></a></li><li><a class="site-page child" href="/2024/08/27/yaml-tutorial/"><i class="fa-fw fas fa-book"></i><span> YAML Tutorial</span></a></li><li><a class="site-page child" href="/categories/Bash-Tutorial/"><i class="fa-fw fas fa-book"></i><span> Bash Tutorial</span></a></li><li><a class="site-page child" href="/2023/09/05/tutorial-gson/"><i class="fa-fw fas fa-music"></i><span> 极简教程 - Gson</span></a></li><li><a class="site-page child" href="/categories/Redis%E6%95%99%E7%A8%8B/"><i class="fa-fw fas fa-book"></i><span> Redis教程</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><span> Dev Wiki</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/2024/11/29/wiki/standard-ascii-table/"><i class="fa-fw fas fa-book"></i><span> Standard ASCII Table (7-bit)</span></a></li><li><a class="site-page child" href="/2025/01/17/a-complete-guide-to-base64-encoding-and-decoding/"><i class="fa-fw fas fa-book"></i><span> Base64 Introduction</span></a></li><li><a class="site-page child" href="/2024/11/30/wiki/html-char/"><i class="fa-fw fas fa-book"></i><span> HTML Special Char</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><span> Dev Tools</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/tools/json-formatter/"><i class="fa-fw fas fa-link"></i><span> JSON Format</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">Ollama Core Concepts</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2025-01-29T04:11:16.000Z" title="Created 2025-01-29 12:11:16">2025-01-29</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2025-02-09T17:36:01.690Z" title="Updated 2025-02-10 01:36:01">2025-02-10</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Ollama-Tutorial/">Ollama Tutorial</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="container post-content" id="article-container"><p>Ollama is a localized machine learning framework designed for various natural language processing (NLP) tasks. It focuses on model loading, inference, and generation, making it easy for users to interact with large pre-trained models deployed locally.</p>
<h2 id="Models"><a href="#Models" class="headerlink" title="Models"></a><strong>Models</strong></h2><p>Models are the heart of Ollama. These are pre-trained machine learning models capable of performing tasks like text generation, summarization, sentiment analysis, and dialogue generation. Ollama supports a wide range of popular pre-trained models, including:</p>
<ul>
<li><strong>Deepseek-v3</strong>: A large language model by DeepSeek, specialized in text generation.</li>
<li><strong>LLama2</strong>: A large language model by Meta, designed for text generation.</li>
<li><strong>GPT</strong>: OpenAI’s GPT series, suitable for dialogue generation, text reasoning, and more.</li>
<li><strong>BERT</strong>: A model for sentence understanding and question-answering systems.</li>
<li><strong>Custom Models</strong>: Users can upload and use their own custom models for inference.</li>
</ul>
<p><strong>Key Features of Models:</strong></p>
<ul>
<li><strong>Inference</strong>: Generating output based on user input.</li>
<li><strong>Fine-tuning</strong>: Adapting pre-trained models to specific tasks or domains using custom datasets.</li>
</ul>
<p>Models are typically neural networks with millions (or billions) of parameters, trained on vast amounts of text data to learn language patterns and perform efficient reasoning.</p>
<p><strong>Supported Models:</strong></p>
<p>Ollama’s library of supported models can be accessed here: <a target="_blank" rel="noopener" href="https://ollama.com/library">Ollama Model Library</a>. </p>
<p><img src="/image/ollama/05-1.png" alt=""></p>
<p>Click on the model to view the download command:</p>
<p><img src="/image/ollama/05-2.png" alt=""></p>
<p>Below are some examples of models and their download commands:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th><strong>Model</strong></th>
<th><strong>Parameters</strong></th>
<th><strong>Size</strong></th>
<th><strong>Download Command</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>DeepSeek-R1</td>
<td>671B</td>
<td>404GB</td>
<td><code>ollama run deepseek-r1:671b</code></td>
</tr>
<tr>
<td>DeepSeek-R1</td>
<td>70B</td>
<td>43GB</td>
<td><code>ollama run deepseek-r1:70b</code></td>
</tr>
<tr>
<td>DeepSeek-R1</td>
<td>32B</td>
<td>20GB</td>
<td><code>ollama run deepseek-r1:32b</code></td>
</tr>
<tr>
<td>DeepSeek-R1</td>
<td>14B</td>
<td>9.0GB</td>
<td><code>ollama run deepseek-r1:14b</code></td>
</tr>
<tr>
<td>DeepSeek-R1</td>
<td>8B</td>
<td>4.9GB</td>
<td><code>ollama run deepseek-r1:8b</code></td>
</tr>
<tr>
<td>DeepSeek-R1</td>
<td>7B</td>
<td>4.7GB</td>
<td><code>ollama run deepseek-r1:7b</code></td>
</tr>
<tr>
<td>DeepSeek-R1</td>
<td>1.5B</td>
<td>1.1GB</td>
<td><code>ollama run deepseek-r1:1.5b</code></td>
</tr>
<tr>
<td>Llama 3.3</td>
<td>70B</td>
<td>43GB</td>
<td><code>ollama run llama3.3</code></td>
</tr>
<tr>
<td>Phi 4</td>
<td>14B</td>
<td>9.1GB</td>
<td><code>ollama run phi4</code></td>
</tr>
<tr>
<td>Llama 3.2</td>
<td>3B</td>
<td>2.0GB</td>
<td><code>ollama run llama3.2</code></td>
</tr>
<tr>
<td>Llama 3.2</td>
<td>1B</td>
<td>1.3GB</td>
<td><code>ollama run llama3.2:1b</code></td>
</tr>
</tbody>
</table>
</div>
<h2 id="Tasks"><a href="#Tasks" class="headerlink" title="Tasks"></a><strong>Tasks</strong></h2><p>Ollama supports a variety of NLP tasks, each corresponding to different use cases for the models. Key tasks include:</p>
<ul>
<li><strong>Chat Generation</strong>: Generating natural dialogue responses in conversations.</li>
<li><strong>Text Generation</strong>: Creating natural language text based on prompts (e.g., articles, stories).</li>
<li><strong>Sentiment Analysis</strong>: Determining the emotional tone of a text (positive, negative, neutral).</li>
<li><strong>Text Summarization</strong>: Condensing long texts into concise summaries.</li>
<li><strong>Translation</strong>: Translating text between languages.</li>
</ul>
<p>Using Ollama’s command-line tools, users can specify tasks and load appropriate models to achieve their goals.</p>
<h2 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a><strong>Inference</strong></h2><p>Inference is the process of feeding input into a trained model to generate output. Ollama provides user-friendly command-line tools and APIs, allowing users to quickly input text and receive results.</p>
<p><strong>How Inference Works:</strong></p>
<ol>
<li><strong>Input</strong>: The user provides text input, such as a question, prompt, or dialogue.</li>
<li><strong>Model Processing</strong>: The model processes the input using its neural network to generate relevant output.</li>
<li><strong>Output</strong>: The model returns the generated text, which could be a response, article, translation, etc.</li>
</ol>
<p>Ollama’s API and CLI make it simple to interact with local models and perform inference tasks efficiently.</p>
<h2 id="Fine-tuning"><a href="#Fine-tuning" class="headerlink" title="Fine-tuning"></a><strong>Fine-tuning</strong></h2><p>Fine-tuning involves further training a pre-trained model on domain-specific data to improve its performance on specific tasks or domains. Ollama supports fine-tuning, enabling users to customize models using their own datasets.</p>
<p><strong>Fine-tuning Process:</strong></p>
<ol>
<li><strong>Prepare Dataset</strong>: Gather and format domain-specific data (e.g., text files or JSON).</li>
<li><strong>Load Pre-trained Model</strong>: Select a suitable pre-trained model (e.g., LLama2 or GPT).</li>
<li><strong>Train</strong>: Fine-tune the model using the custom dataset to adapt it to the target task.</li>
<li><strong>Save and Deploy</strong>: Save the fine-tuned model for future use and deploy it as needed.</li>
</ol>
<p>Fine-tuning helps models achieve higher accuracy and efficiency in specialized tasks.</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="http://linjiangxiong.com">Johnson Lin</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="http://linjiangxiong.com/2025/01/29/ollama-core-concepts/">http://linjiangxiong.com/2025/01/29/ollama-core-concepts/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles on this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless otherwise stated.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Ollama/">Ollama</a><a class="post-meta__tags" href="/tags/LLM/">LLM</a></div><div class="post-share"><div class="social-share" data-image="/image/IMG_3665.JPG" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/01/29/ollama-commands/" title="Ollama Commands Overview"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">Previous</div><div class="info-item-2">Ollama Commands Overview</div></div><div class="info-2"><div class="info-item-1">Ollama CommandsOllama offers a variety of command-line tools (CLI) for interacting with locally running models. To see a list of available commands, you can use: 1ollama --help This will display the following: 12345678910111213141516171819202122232425Large language model runnerUsage:  ollama [flags]  ollama [command]Available Commands:  serve       Start ollama  create      Create a model from a Modelfile  show        Show information for a model  run         Run a model  stop        Stop a...</div></div></div></a><a class="pagination-related" href="/2025/01/29/ollama-cli/" title="Interacting with Ollama Models"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">Next</div><div class="info-item-2">Interacting with Ollama Models</div></div><div class="info-2"><div class="info-item-1">Ollama offers multiple ways to interact with its models, with the most common being through command-line inference operations. Command-Line InteractionThe simplest way to interact with the model is directly through the command line. Running the Model Use the ollama run command to start the model and enter interactive mode: 1ollama run &lt;model-name&gt; For example, to download and run the deepseek-r1:1.5b model: 1ollama run deepseek-r1:1.5b Once the model is running, you can directly input...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2025/01/29/ollama-commands/" title="Ollama Commands Overview"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-01-29</div><div class="info-item-2">Ollama Commands Overview</div></div><div class="info-2"><div class="info-item-1">Ollama CommandsOllama offers a variety of command-line tools (CLI) for interacting with locally running models. To see a list of available commands, you can use: 1ollama --help This will display the following: 12345678910111213141516171819202122232425Large language model runnerUsage:  ollama [flags]  ollama [command]Available Commands:  serve       Start ollama  create      Create a model from a Modelfile  show        Show information for a model  run         Run a model  stop        Stop a...</div></div></div></a><a class="pagination-related" href="/2025/01/29/ollama-intro/" title="Introduction to Ollama"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-01-29</div><div class="info-item-2">Introduction to Ollama</div></div><div class="info-2"><div class="info-item-1">Ollama is an open-source platform for large language models (LLMs), designed to make it easy for users to run, manage, and interact with LLMs directly on their local machines.   It provides a straightforward way to load and use various pre-trained language models, supporting a wide range of natural language processing tasks such as text generation, translation, code writing, and question answering.   What sets Ollama apart is its combination of ready-to-use models and tools with...</div></div></div></a><a class="pagination-related" href="/2025/01/29/ollama-install/" title="Installing Ollama"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-01-29</div><div class="info-item-2">Installing Ollama</div></div><div class="info-2"><div class="info-item-1">Ollama supports multiple operating systems, including macOS, Windows, Linux, and Docker containers.   It has modest hardware requirements, making it easy for users to run, manage, and interact with large language models locally.   Hardware and Software Requirements CPU: A multi-core processor (4 cores or more recommended).   GPU: If you plan to run large models or perform fine-tuning, a GPU with high computational power (e.g., NVIDIA with CUDA support) is recommended.   RAM: At least 8GB of...</div></div></div></a><a class="pagination-related" href="/2025/01/29/ollama-tutorial/" title="Ollama Tutorial"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-01-29</div><div class="info-item-2">Ollama Tutorial</div></div><div class="info-2"><div class="info-item-1"> Ollama is an open-source framework designed to make it easy to deploy and run large language models (LLMs) directly on your local machine. It supports multiple operating systems, including macOS, Windows, Linux, and even Docker containers. One of its standout features is model quantization, which significantly reduces GPU memory requirements, making it possible to run large models on everyday home computers. Who Is This Tutorial For?Ollama is ideal for developers, researchers, and users...</div></div></div></a><a class="pagination-related" href="/2025/01/29/ollama-run-model/" title="Running Models with Ollama"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-01-29</div><div class="info-item-2">Running Models with Ollama</div></div><div class="info-2"><div class="info-item-1">To run a model in Ollama, use the ollama run command.   For example, to run the DeepSeek-R1:8b model and interact with it, use the following command:  1ollama run deepseek-r1:8b If the model isn’t already installed, Ollama will automatically download it.   Once the download is complete, you can interact with the model directly in the terminal:   1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950C:\Users\Administrator&gt;ollama run...</div></div></div></a><a class="pagination-related" href="/2025/01/29/ollama-python-sdk/" title="Using Ollama with Python"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-01-29</div><div class="info-item-2">Using Ollama with Python</div></div><div class="info-2"><div class="info-item-1">Ollama provides a Python SDK that allows you to interact with locally running models directly from your Python environment. This SDK makes it easy to integrate natural language processing tasks into your Python projects, enabling operations like text generation, conversational AI, and model management—all without the need for manual command-line interactions. Installing the Python SDKTo get started, you’ll need to install the Ollama Python SDK. You can do this using pip: 1pip install...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/image/IMG_3665.JPG" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Johnson Lin</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">488</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">63</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">26</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/iuiuu"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/iuiuu" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:me@linjiangxiong.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Models"><span class="toc-number">1.</span> <span class="toc-text">Models</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Tasks"><span class="toc-number">2.</span> <span class="toc-text">Tasks</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Inference"><span class="toc-number">3.</span> <span class="toc-text">Inference</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Fine-tuning"><span class="toc-number">4.</span> <span class="toc-text">Fine-tuning</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Posts</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/02/06/how-to-run-deepseek-locally-on-windows/" title="How To Run DeepSeek Locally On Windows?">How To Run DeepSeek Locally On Windows?</a><time datetime="2025-02-05T18:27:11.000Z" title="Created 2025-02-06 02:27:11">2025-02-06</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/01/29/ollama-python-sdk/" title="Using Ollama with Python">Using Ollama with Python</a><time datetime="2025-01-29T07:17:04.000Z" title="Created 2025-01-29 15:17:04">2025-01-29</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/01/29/ollama-api/" title="Interacting with the Ollama API">Interacting with the Ollama API</a><time datetime="2025-01-29T06:01:52.000Z" title="Created 2025-01-29 14:01:52">2025-01-29</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/01/29/ollama-cli/" title="Interacting with Ollama Models">Interacting with Ollama Models</a><time datetime="2025-01-29T05:06:06.000Z" title="Created 2025-01-29 13:06:06">2025-01-29</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/01/29/ollama-core-concepts/" title="Ollama Core Concepts">Ollama Core Concepts</a><time datetime="2025-01-29T04:11:16.000Z" title="Created 2025-01-29 12:11:16">2025-01-29</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2025 By Johnson Lin</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Reading Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light and Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle Between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Settings"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back to Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="https://cdn.jsdelivr.net/npm/hexo-theme-butterfly/source/js/utils.min.js"></script><script src="https://cdn.jsdelivr.net/npm/hexo-theme-butterfly/source/js/main.min.js"></script><div class="js-pjax"></div><script defer="defer" id="fluttering_ribbon" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-fluttering-ribbon.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>