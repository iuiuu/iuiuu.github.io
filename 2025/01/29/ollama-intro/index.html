<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Introduction to Ollama | Johnson Lin</title><meta name="author" content="Johnson Lin"><meta name="copyright" content="Johnson Lin"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Ollama is an open-source platform for large language models (LLMs), designed to make it easy for users to run, manage, and interact with LLMs directly on their local machines.   It provides a straight">
<meta property="og:type" content="article">
<meta property="og:title" content="Introduction to Ollama">
<meta property="og:url" content="http://linjiangxiong.com/2025/01/29/ollama-intro/index.html">
<meta property="og:site_name" content="Johnson Lin">
<meta property="og:description" content="Ollama is an open-source platform for large language models (LLMs), designed to make it easy for users to run, manage, and interact with LLMs directly on their local machines.   It provides a straight">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://linjiangxiong.com/image/IMG_3665.JPG">
<meta property="article:published_time" content="2025-01-28T16:45:50.000Z">
<meta property="article:modified_time" content="2025-02-10T17:37:06.075Z">
<meta property="article:author" content="Johnson Lin">
<meta property="article:tag" content="Ollama">
<meta property="article:tag" content="LLM">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://linjiangxiong.com/image/IMG_3665.JPG"><link rel="shortcut icon" href="/image/IMG_3665.JPG"><link rel="canonical" href="http://linjiangxiong.com/2025/01/29/ollama-intro/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Nunito:wght@400;700&amp;family=Source+Sans+3:wght@400;600&amp;display=swap"><link rel="stylesheet" href="/css/ud_v5.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?ca45da0012a3ce293c6ca4f7e5ebc3a8";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
btf.addGlobalFn('pjaxComplete', () => {
  _hmt.push(['_trackPageview',window.location.pathname])
}, 'baidu_analytics')
</script><script async="async" src="https://www.googletagmanager.com/gtag/js?id=WLKqFH01ST8bHfxLtjsterJZnoEpVlF26sn-Nzzoqfc"></script><script>window.dataLayer = window.dataLayer || []
function gtag(){dataLayer.push(arguments)}
gtag('js', new Date())
gtag('config', 'WLKqFH01ST8bHfxLtjsterJZnoEpVlF26sn-Nzzoqfc')
btf.addGlobalFn('pjaxComplete', () => {
  gtag('config', 'WLKqFH01ST8bHfxLtjsterJZnoEpVlF26sn-Nzzoqfc', {'page_path': window.location.pathname})
}, 'google_analytics')
</script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Failed',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Introduction to Ollama',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  isShuoshuo: false
}</script><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.2.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/image/IMG_3665.JPG" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">488</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">63</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">26</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> Instant Tutorials</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/2025/01/29/ollama-tutorial/"><i class="fa-fw fas fa-book"></i><span> Ollama Tutorial</span></a></li><li><a class="site-page child" href="/2024/06/29/html-instant-tutorial/"><i class="fa-fw fas fa-book"></i><span> HTML Tutorial</span></a></li><li><a class="site-page child" href="/2024/08/27/yaml-tutorial/"><i class="fa-fw fas fa-book"></i><span> YAML Tutorial</span></a></li><li><a class="site-page child" href="/categories/Bash-Tutorial/"><i class="fa-fw fas fa-book"></i><span> Bash Tutorial</span></a></li><li><a class="site-page child" href="/2023/09/05/tutorial-gson/"><i class="fa-fw fas fa-music"></i><span> 极简教程 - Gson</span></a></li><li><a class="site-page child" href="/categories/Redis%E6%95%99%E7%A8%8B/"><i class="fa-fw fas fa-book"></i><span> Redis教程</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><span> Dev Wiki</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/2024/11/29/wiki/standard-ascii-table/"><i class="fa-fw fas fa-book"></i><span> Standard ASCII Table (7-bit)</span></a></li><li><a class="site-page child" href="/2025/01/17/a-complete-guide-to-base64-encoding-and-decoding/"><i class="fa-fw fas fa-book"></i><span> Base64 Introduction</span></a></li><li><a class="site-page child" href="/2024/11/30/wiki/html-char/"><i class="fa-fw fas fa-book"></i><span> HTML Special Char</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><span> Dev Tools</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/tools/json-formatter/"><i class="fa-fw fas fa-link"></i><span> JSON Format</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">Johnson Lin</span></a><a class="nav-page-title" href="/"><span class="site-name">Introduction to Ollama</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> Instant Tutorials</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/2025/01/29/ollama-tutorial/"><i class="fa-fw fas fa-book"></i><span> Ollama Tutorial</span></a></li><li><a class="site-page child" href="/2024/06/29/html-instant-tutorial/"><i class="fa-fw fas fa-book"></i><span> HTML Tutorial</span></a></li><li><a class="site-page child" href="/2024/08/27/yaml-tutorial/"><i class="fa-fw fas fa-book"></i><span> YAML Tutorial</span></a></li><li><a class="site-page child" href="/categories/Bash-Tutorial/"><i class="fa-fw fas fa-book"></i><span> Bash Tutorial</span></a></li><li><a class="site-page child" href="/2023/09/05/tutorial-gson/"><i class="fa-fw fas fa-music"></i><span> 极简教程 - Gson</span></a></li><li><a class="site-page child" href="/categories/Redis%E6%95%99%E7%A8%8B/"><i class="fa-fw fas fa-book"></i><span> Redis教程</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><span> Dev Wiki</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/2024/11/29/wiki/standard-ascii-table/"><i class="fa-fw fas fa-book"></i><span> Standard ASCII Table (7-bit)</span></a></li><li><a class="site-page child" href="/2025/01/17/a-complete-guide-to-base64-encoding-and-decoding/"><i class="fa-fw fas fa-book"></i><span> Base64 Introduction</span></a></li><li><a class="site-page child" href="/2024/11/30/wiki/html-char/"><i class="fa-fw fas fa-book"></i><span> HTML Special Char</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><span> Dev Tools</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/tools/json-formatter/"><i class="fa-fw fas fa-link"></i><span> JSON Format</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">Introduction to Ollama</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2025-01-28T16:45:50.000Z" title="Created 2025-01-29 00:45:50">2025-01-29</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2025-02-10T17:37:06.075Z" title="Updated 2025-02-11 01:37:06">2025-02-11</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Ollama-Tutorial/">Ollama Tutorial</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="container post-content" id="article-container"><p>Ollama is an open-source platform for large language models (LLMs), designed to make it easy for users to run, manage, and interact with LLMs directly on their local machines.  </p>
<p>It provides a straightforward way to load and use various pre-trained language models, supporting a wide range of natural language processing tasks such as text generation, translation, code writing, and question answering.  </p>
<p>What sets Ollama apart is its combination of ready-to-use models and tools with user-friendly interfaces and APIs. This enables quick implementation of tasks like text generation, dialogue systems, and semantic analysis.  </p>
<p>Unlike other NLP frameworks, Ollama simplifies workflows, making machine learning accessible to users without deep technical expertise.  </p>
<p>Additionally, Ollama supports multiple hardware acceleration options, including CPU-only inference and various underlying architectures (such as Apple Silicon), ensuring efficient utilization of diverse hardware resources.</p>
<h2 id="Core-Features-and-Highlights"><a href="#Core-Features-and-Highlights" class="headerlink" title="Core Features and Highlights"></a>Core Features and Highlights</h2><ul>
<li><strong>Support for Multiple Pre-trained Language Models</strong><br>Ollama offers a variety of ready-to-use pre-trained models, including popular large language models like GPT and BERT. Users can easily load and utilize these models for tasks such as text generation, sentiment analysis, and question answering.  </li>
<li><strong>Easy Integration and Usability</strong><br>Ollama provides a command-line interface (CLI) and Python SDK, simplifying integration with other projects and services. Developers can seamlessly incorporate Ollama into their existing applications without dealing with complex dependencies or configurations.  </li>
<li><strong>Local Deployment and Offline Usage</strong><br>Unlike cloud-based NLP services, Ollama allows developers to run models in local computing environments. This eliminates reliance on external servers, ensures data privacy, and delivers lower latency and greater control for high-concurrency requests.  </li>
<li><strong>Model Fine-tuning and Customization</strong><br>In addition to using pre-trained models, users can fine-tune models to suit specific needs. Developers can retrain models using their own datasets to enhance performance and accuracy.  </li>
<li><strong>Performance Optimization</strong><br>Ollama prioritizes efficiency with optimized inference mechanisms and support for batch processing. It effectively manages memory and computational resources, ensuring high performance even with large-scale data.  </li>
<li><strong>Cross-platform Support</strong><br>Ollama is compatible with multiple operating systems, including Windows, macOS, and Linux. This ensures a consistent experience whether developers are debugging locally or deploying in production environments.  </li>
<li><strong>Open Source and Community Support</strong><br>As an open-source project, Ollama allows developers to access, modify, and improve its codebase. It also boasts an active community where users can seek help, share insights, and collaborate on the project.</li>
</ul>
<h2 id="Use-Cases"><a href="#Use-Cases" class="headerlink" title="Use Cases"></a>Use Cases</h2><ul>
<li><strong>Content Creation</strong><br>Assists writers, journalists, and marketers in quickly generating high-quality content, such as blog posts, ad copy, and more.  </li>
<li><strong>Programming Assistance</strong><br>Helps developers generate code, debug programs, or optimize code structures.  </li>
<li><strong>Education and Research</strong><br>Supports students and researchers in learning, writing, and conducting studies, such as generating paper summaries or answering questions.  </li>
<li><strong>Cross-language Communication</strong><br>Provides high-quality translation capabilities, breaking down language barriers for users.  </li>
<li><strong>Personal Assistant</strong><br>Acts as a smart assistant, helping users with daily tasks like drafting emails or creating to-do lists.</li>
</ul>
<h2 id="How-Ollama-Differs-from-Other-LLMs"><a href="#How-Ollama-Differs-from-Other-LLMs" class="headerlink" title="How Ollama Differs from Other LLMs"></a>How Ollama Differs from Other LLMs</h2><div class="table-container">
<table>
<thead>
<tr>
<th><strong>Aspect</strong></th>
<th><strong>Ollama’s Features</strong></th>
<th><strong>Explanation</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Localization</strong></td>
<td>Focuses on local execution</td>
<td>Unlike cloud-dependent LLMs like ChatGPT, Ollama is ideal for users with high data privacy concerns.</td>
</tr>
<tr>
<td><strong>Flexibility</strong></td>
<td>Supports loading different models</td>
<td>Users can load various models based on their needs, without being limited to a single model.</td>
</tr>
<tr>
<td><strong>Open Source</strong></td>
<td>Fully open-source</td>
<td>Users can freely modify and extend its functionality to suit their specific requirements.</td>
</tr>
</tbody>
</table>
</div>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="http://linjiangxiong.com">Johnson Lin</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="http://linjiangxiong.com/2025/01/29/ollama-intro/">http://linjiangxiong.com/2025/01/29/ollama-intro/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles on this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless otherwise stated.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Ollama/">Ollama</a><a class="post-meta__tags" href="/tags/LLM/">LLM</a></div><div class="post-share"><div class="social-share" data-image="/image/IMG_3665.JPG" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/01/29/ollama-tutorial/" title="Ollama Tutorial"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">Previous</div><div class="info-item-2">Ollama Tutorial</div></div><div class="info-2"><div class="info-item-1"> Ollama is an open-source framework designed to make it easy to deploy and run large language models (LLMs) directly on your local machine. It supports multiple operating systems, including macOS, Windows, Linux, and even Docker containers. One of its standout features is model quantization, which significantly reduces GPU memory requirements, making it possible to run large models on everyday home computers. Who Is This Tutorial For?Ollama is ideal for developers, researchers, and users...</div></div></div></a><a class="pagination-related" href="/2025/01/29/ollama-install/" title="Installing Ollama"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">Next</div><div class="info-item-2">Installing Ollama</div></div><div class="info-2"><div class="info-item-1">Ollama supports multiple operating systems, including macOS, Windows, Linux, and Docker containers.   It has modest hardware requirements, making it easy for users to run, manage, and interact with large language models locally.   Hardware and Software Requirements CPU: A multi-core processor (4 cores or more recommended).   GPU: If you plan to run large models or perform fine-tuning, a GPU with high computational power (e.g., NVIDIA with CUDA support) is recommended.   RAM: At least 8GB of...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2025/01/29/ollama-commands/" title="Ollama Commands Overview"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-01-29</div><div class="info-item-2">Ollama Commands Overview</div></div><div class="info-2"><div class="info-item-1">Ollama CommandsOllama offers a variety of command-line tools (CLI) for interacting with locally running models. To see a list of available commands, you can use: 1ollama --help This will display the following: 12345678910111213141516171819202122232425Large language model runnerUsage:  ollama [flags]  ollama [command]Available Commands:  serve       Start ollama  create      Create a model from a Modelfile  show        Show information for a model  run         Run a model  stop        Stop a...</div></div></div></a><a class="pagination-related" href="/2025/01/29/ollama-install/" title="Installing Ollama"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-01-29</div><div class="info-item-2">Installing Ollama</div></div><div class="info-2"><div class="info-item-1">Ollama supports multiple operating systems, including macOS, Windows, Linux, and Docker containers.   It has modest hardware requirements, making it easy for users to run, manage, and interact with large language models locally.   Hardware and Software Requirements CPU: A multi-core processor (4 cores or more recommended).   GPU: If you plan to run large models or perform fine-tuning, a GPU with high computational power (e.g., NVIDIA with CUDA support) is recommended.   RAM: At least 8GB of...</div></div></div></a><a class="pagination-related" href="/2025/01/29/ollama-tutorial/" title="Ollama Tutorial"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-01-29</div><div class="info-item-2">Ollama Tutorial</div></div><div class="info-2"><div class="info-item-1"> Ollama is an open-source framework designed to make it easy to deploy and run large language models (LLMs) directly on your local machine. It supports multiple operating systems, including macOS, Windows, Linux, and even Docker containers. One of its standout features is model quantization, which significantly reduces GPU memory requirements, making it possible to run large models on everyday home computers. Who Is This Tutorial For?Ollama is ideal for developers, researchers, and users...</div></div></div></a><a class="pagination-related" href="/2025/01/29/ollama-run-model/" title="Running Models with Ollama"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-01-29</div><div class="info-item-2">Running Models with Ollama</div></div><div class="info-2"><div class="info-item-1">To run a model in Ollama, use the ollama run command.   For example, to run the DeepSeek-R1:8b model and interact with it, use the following command:  1ollama run deepseek-r1:8b If the model isn’t already installed, Ollama will automatically download it.   Once the download is complete, you can interact with the model directly in the terminal:   1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950C:\Users\Administrator&gt;ollama run...</div></div></div></a><a class="pagination-related" href="/2025/01/29/ollama-core-concepts/" title="Ollama Core Concepts"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-01-29</div><div class="info-item-2">Ollama Core Concepts</div></div><div class="info-2"><div class="info-item-1">Ollama is a localized machine learning framework designed for various natural language processing (NLP) tasks. It focuses on model loading, inference, and generation, making it easy for users to interact with large pre-trained models deployed locally. ModelsModels are the heart of Ollama. These are pre-trained machine learning models capable of performing tasks like text generation, summarization, sentiment analysis, and dialogue generation. Ollama supports a wide range of popular...</div></div></div></a><a class="pagination-related" href="/2025/01/29/ollama-python-sdk/" title="Using Ollama with Python"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-01-29</div><div class="info-item-2">Using Ollama with Python</div></div><div class="info-2"><div class="info-item-1">Ollama provides a Python SDK that allows you to interact with locally running models directly from your Python environment. This SDK makes it easy to integrate natural language processing tasks into your Python projects, enabling operations like text generation, conversational AI, and model management—all without the need for manual command-line interactions. Installing the Python SDKTo get started, you’ll need to install the Ollama Python SDK. You can do this using pip: 1pip install...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/image/IMG_3665.JPG" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Johnson Lin</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">488</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">63</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">26</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/iuiuu"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/iuiuu" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:me@linjiangxiong.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Core-Features-and-Highlights"><span class="toc-number">1.</span> <span class="toc-text">Core Features and Highlights</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Use-Cases"><span class="toc-number">2.</span> <span class="toc-text">Use Cases</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#How-Ollama-Differs-from-Other-LLMs"><span class="toc-number">3.</span> <span class="toc-text">How Ollama Differs from Other LLMs</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Posts</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/02/06/how-to-run-deepseek-locally-on-windows/" title="How To Run DeepSeek Locally On Windows?">How To Run DeepSeek Locally On Windows?</a><time datetime="2025-02-05T18:27:11.000Z" title="Created 2025-02-06 02:27:11">2025-02-06</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/01/29/ollama-python-sdk/" title="Using Ollama with Python">Using Ollama with Python</a><time datetime="2025-01-29T07:17:04.000Z" title="Created 2025-01-29 15:17:04">2025-01-29</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/01/29/ollama-api/" title="Interacting with the Ollama API">Interacting with the Ollama API</a><time datetime="2025-01-29T06:01:52.000Z" title="Created 2025-01-29 14:01:52">2025-01-29</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/01/29/ollama-cli/" title="Interacting with Ollama Models">Interacting with Ollama Models</a><time datetime="2025-01-29T05:06:06.000Z" title="Created 2025-01-29 13:06:06">2025-01-29</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/01/29/ollama-core-concepts/" title="Ollama Core Concepts">Ollama Core Concepts</a><time datetime="2025-01-29T04:11:16.000Z" title="Created 2025-01-29 12:11:16">2025-01-29</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2025 By Johnson Lin</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Reading Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light and Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle Between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Settings"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back to Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="https://cdn.jsdelivr.net/npm/hexo-theme-butterfly/source/js/utils.min.js"></script><script src="https://cdn.jsdelivr.net/npm/hexo-theme-butterfly/source/js/main.min.js"></script><div class="js-pjax"></div><script defer="defer" id="fluttering_ribbon" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-fluttering-ribbon.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>