<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Using Ollama with Python | Johnson Lin</title><meta name="author" content="Johnson Lin"><meta name="copyright" content="Johnson Lin"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Ollama provides a Python SDK that allows you to interact with locally running models directly from your Python environment. This SDK makes it easy to integrate natural language processing tasks into y">
<meta property="og:type" content="article">
<meta property="og:title" content="Using Ollama with Python">
<meta property="og:url" content="http://linjiangxiong.com/2025/01/29/ollama-python-sdk/index.html">
<meta property="og:site_name" content="Johnson Lin">
<meta property="og:description" content="Ollama provides a Python SDK that allows you to interact with locally running models directly from your Python environment. This SDK makes it easy to integrate natural language processing tasks into y">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://linjiangxiong.com/image/IMG_3665.JPG">
<meta property="article:published_time" content="2025-01-29T07:17:04.000Z">
<meta property="article:modified_time" content="2025-02-09T17:23:14.655Z">
<meta property="article:author" content="Johnson Lin">
<meta property="article:tag" content="Ollama">
<meta property="article:tag" content="LLM">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://linjiangxiong.com/image/IMG_3665.JPG"><link rel="shortcut icon" href="/image/IMG_3665.JPG"><link rel="canonical" href="http://linjiangxiong.com/2025/01/29/ollama-python-sdk/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Nunito:wght@400;700&amp;family=Source+Sans+3:wght@400;600&amp;display=swap"><link rel="stylesheet" href="/css/ud_v5.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?ca45da0012a3ce293c6ca4f7e5ebc3a8";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
btf.addGlobalFn('pjaxComplete', () => {
  _hmt.push(['_trackPageview',window.location.pathname])
}, 'baidu_analytics')
</script><script async="async" src="https://www.googletagmanager.com/gtag/js?id=WLKqFH01ST8bHfxLtjsterJZnoEpVlF26sn-Nzzoqfc"></script><script>window.dataLayer = window.dataLayer || []
function gtag(){dataLayer.push(arguments)}
gtag('js', new Date())
gtag('config', 'WLKqFH01ST8bHfxLtjsterJZnoEpVlF26sn-Nzzoqfc')
btf.addGlobalFn('pjaxComplete', () => {
  gtag('config', 'WLKqFH01ST8bHfxLtjsterJZnoEpVlF26sn-Nzzoqfc', {'page_path': window.location.pathname})
}, 'google_analytics')
</script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Failed',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Using Ollama with Python',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  isShuoshuo: false
}</script><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.2.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/image/IMG_3665.JPG" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">486</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">62</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">25</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> Instant Tutorials</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/2025/01/29/ollama-tutorial/"><i class="fa-fw fas fa-book"></i><span> Ollama Tutorial</span></a></li><li><a class="site-page child" href="/2024/06/29/html-instant-tutorial/"><i class="fa-fw fas fa-book"></i><span> HTML Tutorial</span></a></li><li><a class="site-page child" href="/2024/08/27/yaml-tutorial/"><i class="fa-fw fas fa-book"></i><span> YAML Tutorial</span></a></li><li><a class="site-page child" href="/categories/Bash-Tutorial/"><i class="fa-fw fas fa-book"></i><span> Bash Tutorial</span></a></li><li><a class="site-page child" href="/2023/09/05/tutorial-gson/"><i class="fa-fw fas fa-music"></i><span> 极简教程 - Gson</span></a></li><li><a class="site-page child" href="/categories/Redis%E6%95%99%E7%A8%8B/"><i class="fa-fw fas fa-book"></i><span> Redis教程</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><span> Dev Wiki</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/2024/11/29/wiki/standard-ascii-table/"><i class="fa-fw fas fa-book"></i><span> Standard ASCII Table (7-bit)</span></a></li><li><a class="site-page child" href="/2025/01/17/a-complete-guide-to-base64-encoding-and-decoding/"><i class="fa-fw fas fa-book"></i><span> Base64 Introduction</span></a></li><li><a class="site-page child" href="/2024/11/30/wiki/html-char/"><i class="fa-fw fas fa-book"></i><span> HTML Special Char</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><span> Dev Tools</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/tools/json-formatter/"><i class="fa-fw fas fa-link"></i><span> JSON Format</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">Johnson Lin</span></a><a class="nav-page-title" href="/"><span class="site-name">Using Ollama with Python</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> Instant Tutorials</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/2025/01/29/ollama-tutorial/"><i class="fa-fw fas fa-book"></i><span> Ollama Tutorial</span></a></li><li><a class="site-page child" href="/2024/06/29/html-instant-tutorial/"><i class="fa-fw fas fa-book"></i><span> HTML Tutorial</span></a></li><li><a class="site-page child" href="/2024/08/27/yaml-tutorial/"><i class="fa-fw fas fa-book"></i><span> YAML Tutorial</span></a></li><li><a class="site-page child" href="/categories/Bash-Tutorial/"><i class="fa-fw fas fa-book"></i><span> Bash Tutorial</span></a></li><li><a class="site-page child" href="/2023/09/05/tutorial-gson/"><i class="fa-fw fas fa-music"></i><span> 极简教程 - Gson</span></a></li><li><a class="site-page child" href="/categories/Redis%E6%95%99%E7%A8%8B/"><i class="fa-fw fas fa-book"></i><span> Redis教程</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><span> Dev Wiki</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/2024/11/29/wiki/standard-ascii-table/"><i class="fa-fw fas fa-book"></i><span> Standard ASCII Table (7-bit)</span></a></li><li><a class="site-page child" href="/2025/01/17/a-complete-guide-to-base64-encoding-and-decoding/"><i class="fa-fw fas fa-book"></i><span> Base64 Introduction</span></a></li><li><a class="site-page child" href="/2024/11/30/wiki/html-char/"><i class="fa-fw fas fa-book"></i><span> HTML Special Char</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><span> Dev Tools</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/tools/json-formatter/"><i class="fa-fw fas fa-link"></i><span> JSON Format</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">Using Ollama with Python</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2025-01-29T07:17:04.000Z" title="Created 2025-01-29 15:17:04">2025-01-29</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2025-02-09T17:23:14.655Z" title="Updated 2025-02-10 01:23:14">2025-02-10</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Ollama-Tutorial/">Ollama Tutorial</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="container post-content" id="article-container"><p>Ollama provides a Python SDK that allows you to interact with locally running models directly from your Python environment. This SDK makes it easy to integrate natural language processing tasks into your Python projects, enabling operations like text generation, conversational AI, and model management—all without the need for manual command-line interactions.</p>
<h2 id="Installing-the-Python-SDK"><a href="#Installing-the-Python-SDK" class="headerlink" title="Installing the Python SDK"></a>Installing the Python SDK</h2><p>To get started, you’ll need to install the Ollama Python SDK. You can do this using <code>pip</code>:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install ollama</span><br></pre></td></tr></table></figure>
<p>Make sure you have Python 3.x installed and that your environment can access the Ollama local service.</p>
<h2 id="Starting-the-Local-Service"><a href="#Starting-the-Local-Service" class="headerlink" title="Starting the Local Service"></a>Starting the Local Service</h2><p>Before using the Python SDK, ensure that the Ollama local service is up and running. You can start it using the command line:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ollama serve</span><br></pre></td></tr></table></figure>
<p>Once the local service is running, the Python SDK will communicate with it to perform tasks like model inference.</p>
<h2 id="Using-the-Ollama-Python-SDK-for-Inference"><a href="#Using-the-Ollama-Python-SDK-for-Inference" class="headerlink" title="Using the Ollama Python SDK for Inference"></a>Using the Ollama Python SDK for Inference</h2><p>After installing the SDK and starting the local service, you can interact with Ollama using Python code. Here’s how:</p>
<ol>
<li>Import the necessary modules:</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> ollama <span class="keyword">import</span> chat, ChatResponse</span><br></pre></td></tr></table></figure>
<ol>
<li>Send requests to a specified model to generate text or dialogue:</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">response: ChatResponse = chat(model=<span class="string">&#x27;deepseek-r1:1.5b&#x27;</span>, messages=[</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&#x27;role&#x27;</span>: <span class="string">&#x27;user&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;content&#x27;</span>: <span class="string">&#x27;Who are you?&#x27;</span>,</span><br><span class="line">    &#125;,</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print the response content</span></span><br><span class="line"><span class="built_in">print</span>(response[<span class="string">&#x27;message&#x27;</span>][<span class="string">&#x27;content&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Alternatively, access the response object directly</span></span><br><span class="line"><span class="comment"># print(response.message.content)</span></span><br></pre></td></tr></table></figure>
<p>When you run this code, the output might look like this:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Greetings! I&#x27;m DeepSeek-R1, an artificial intelligence assistant created by DeepSeek. I&#x27;m at your service and would be delighted to assist you with any inquiries or tasks you may have.</span><br></pre></td></tr></table></figure>
<p><strong>Streaming Responses</strong></p>
<p>The Ollama SDK also supports streaming responses. You can enable this by setting <code>stream=True</code> when sending a request:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">stream = chat(</span><br><span class="line">    model=<span class="string">&#x27;deepseek-r1:1.5b&#x27;</span>,</span><br><span class="line">    messages=[&#123;<span class="string">&#x27;role&#x27;</span>: <span class="string">&#x27;user&#x27;</span>, <span class="string">&#x27;content&#x27;</span>: <span class="string">&#x27;Who are you?&#x27;</span>&#125;],</span><br><span class="line">    stream=<span class="literal">True</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print the response in chunks</span></span><br><span class="line"><span class="keyword">for</span> chunk <span class="keyword">in</span> stream:</span><br><span class="line">    <span class="built_in">print</span>(chunk[<span class="string">&#x27;message&#x27;</span>][<span class="string">&#x27;content&#x27;</span>], end=<span class="string">&#x27;&#x27;</span>, flush=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h2 id="Custom-Clients"><a href="#Custom-Clients" class="headerlink" title="Custom Clients"></a>Custom Clients</h2><p>For more control over request configurations, such as custom headers or specifying the local service URL, you can create a custom client:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> ollama <span class="keyword">import</span> Client</span><br><span class="line"></span><br><span class="line">client = Client(</span><br><span class="line">    host=<span class="string">&#x27;http://localhost:11434&#x27;</span>,</span><br><span class="line">    headers=&#123;<span class="string">&#x27;x-some-header&#x27;</span>: <span class="string">&#x27;some-value&#x27;</span>&#125;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">response = client.chat(model=<span class="string">&#x27;deepseek-r1:1.5b&#x27;</span>, messages=[</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&#x27;role&#x27;</span>: <span class="string">&#x27;user&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;content&#x27;</span>: <span class="string">&#x27;Who are you?&#x27;</span>,</span><br><span class="line">    &#125;,</span><br><span class="line">])</span><br><span class="line"><span class="built_in">print</span>(response[<span class="string">&#x27;message&#x27;</span>][<span class="string">&#x27;content&#x27;</span>])</span><br></pre></td></tr></table></figure>
<hr>
<p><strong>Asynchronous Clients</strong></p>
<p>If you need to handle requests asynchronously, you can use the <code>AsyncClient</code> class, which is ideal for high-concurrency scenarios:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">from</span> ollama <span class="keyword">import</span> AsyncClient</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">chat</span>():</span><br><span class="line">    message = &#123;<span class="string">&#x27;role&#x27;</span>: <span class="string">&#x27;user&#x27;</span>, <span class="string">&#x27;content&#x27;</span>: <span class="string">&#x27;Who are you?&#x27;</span>&#125;</span><br><span class="line">    response = <span class="keyword">await</span> AsyncClient().chat(model=<span class="string">&#x27;deepseek-r1:1.5b&#x27;</span>, messages=[message])</span><br><span class="line">    <span class="built_in">print</span>(response[<span class="string">&#x27;message&#x27;</span>][<span class="string">&#x27;content&#x27;</span>])</span><br><span class="line"></span><br><span class="line">asyncio.run(chat())</span><br></pre></td></tr></table></figure>
<p><strong>Asynchronous Streaming</strong></p>
<p>For asynchronous streaming responses, you can use an asynchronous generator:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">from</span> ollama <span class="keyword">import</span> AsyncClient</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">chat</span>():</span><br><span class="line">    message = &#123;<span class="string">&#x27;role&#x27;</span>: <span class="string">&#x27;user&#x27;</span>, <span class="string">&#x27;content&#x27;</span>: <span class="string">&#x27;Who are you?&#x27;</span>&#125;</span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">for</span> part <span class="keyword">in</span> <span class="keyword">await</span> AsyncClient().chat(model=<span class="string">&#x27;deepseek-r1:1.5b&#x27;</span>, messages=[message], stream=<span class="literal">True</span>):</span><br><span class="line">        <span class="built_in">print</span>(part[<span class="string">&#x27;message&#x27;</span>][<span class="string">&#x27;content&#x27;</span>], end=<span class="string">&#x27;&#x27;</span>, flush=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">asyncio.run(chat())</span><br></pre></td></tr></table></figure>
<p>Here, the response is returned in parts asynchronously, allowing you to process each part immediately.</p>
<h2 id="Common-API-Methods"><a href="#Common-API-Methods" class="headerlink" title="Common API Methods"></a>Common API Methods</h2><p>The Ollama Python SDK provides several useful API methods for managing and interacting with models:</p>
<ol>
<li><strong>Chat</strong>: Generate conversational responses.</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ollama.chat(model=<span class="string">&#x27;deepseek-r1:1.5b&#x27;</span>, messages=[&#123;<span class="string">&#x27;role&#x27;</span>: <span class="string">&#x27;user&#x27;</span>, <span class="string">&#x27;content&#x27;</span>: <span class="string">&#x27;Why is the sky blue?&#x27;</span>&#125;])</span><br></pre></td></tr></table></figure>
<ol>
<li><strong>Generate</strong>: Generate text based on a prompt.</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ollama.generate(model=<span class="string">&#x27;deepseek-r1:1.5b&#x27;</span>, prompt=<span class="string">&#x27;Why is the sky blue?&#x27;</span>)</span><br></pre></td></tr></table></figure>
<ol>
<li><strong>List</strong>: List all available models.</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ollama.<span class="built_in">list</span>()</span><br></pre></td></tr></table></figure>
<ol>
<li><strong>Show</strong>: Display details about a specific model.</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ollama.show(<span class="string">&#x27;deepseek-r1:1.5b&#x27;</span>)</span><br></pre></td></tr></table></figure>
<ol>
<li><strong>Create</strong>: Create a new model from an existing one.</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ollama.create(model=<span class="string">&#x27;example&#x27;</span>, from_=<span class="string">&#x27;deepseek-r1:1.5b&#x27;</span>, system=<span class="string">&quot;You are Mario from Super Mario Bros.&quot;</span>)</span><br></pre></td></tr></table></figure>
<ol>
<li><strong>Copy</strong>: Copy a model to another location.</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ollama.copy(<span class="string">&#x27;deepseek-r1:1.5b&#x27;</span>, <span class="string">&#x27;user/deepseek-r1:1.5b&#x27;</span>)</span><br></pre></td></tr></table></figure>
<ol>
<li><strong>Delete</strong>: Delete a model.</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ollama.delete(<span class="string">&#x27;deepseek-r1:1.5b&#x27;</span>)</span><br></pre></td></tr></table></figure>
<ol>
<li><strong>Pull</strong>: Download a model from a remote repository.</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ollama.pull(<span class="string">&#x27;deepseek-r1:1.5b&#x27;</span>)</span><br></pre></td></tr></table></figure>
<ol>
<li><strong>Push</strong>: Upload a model to a remote repository.</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ollama.push(<span class="string">&#x27;user/deepseek-r1:1.5b&#x27;</span>)</span><br></pre></td></tr></table></figure>
<ol>
<li><strong>Embed</strong>: Generate text embeddings.</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ollama.embed(model=<span class="string">&#x27;deepseek-r1:1.5b&#x27;</span>, <span class="built_in">input</span>=<span class="string">&#x27;The sky is blue because of Rayleigh scattering&#x27;</span>)</span><br></pre></td></tr></table></figure>
<ol>
<li><strong>Ps</strong>: List running models.</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ollama.ps()</span><br></pre></td></tr></table></figure>
<h2 id="Error-Handling"><a href="#Error-Handling" class="headerlink" title="Error Handling"></a>Error Handling</h2><p>The Ollama SDK raises errors when requests fail or streaming issues occur. You can handle these errors using <code>try-except</code> blocks:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> ollama</span><br><span class="line"></span><br><span class="line">model = <span class="string">&#x27;does-not-yet-exist&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = ollama.chat(model)</span><br><span class="line"><span class="keyword">except</span> ollama.ResponseError <span class="keyword">as</span> e:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Error:&#x27;</span>, e.error)</span><br><span class="line">    <span class="keyword">if</span> e.status_code == <span class="number">404</span>:</span><br><span class="line">        ollama.pull(model)</span><br></pre></td></tr></table></figure>
<p>In this example, if the model doesn’t exist, a <code>ResponseError</code> is raised, and you can choose to pull the model or handle the error accordingly.</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="http://linjiangxiong.com">Johnson Lin</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="http://linjiangxiong.com/2025/01/29/ollama-python-sdk/">http://linjiangxiong.com/2025/01/29/ollama-python-sdk/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles on this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless otherwise stated.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Ollama/">Ollama</a><a class="post-meta__tags" href="/tags/LLM/">LLM</a></div><div class="post-share"><div class="social-share" data-image="/image/IMG_3665.JPG" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related full-width" href="/2025/01/29/ollama-cli/" title="Interacting with Ollama Models"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">Next</div><div class="info-item-2">Interacting with Ollama Models</div></div><div class="info-2"><div class="info-item-1">Ollama offers multiple ways to interact with its models, with the most common being through command-line inference operations. Command-Line InteractionThe simplest way to interact with the model is directly through the command line. Running the Model Use the ollama run command to start the model and enter interactive mode: 1ollama run &lt;model-name&gt; For example, to download and run the deepseek-r1:1.5b model: 1ollama run deepseek-r1:1.5b Once the model is running, you can directly input...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2025/01/29/ollama-commands/" title="Ollama Commands Overview"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-01-29</div><div class="info-item-2">Ollama Commands Overview</div></div><div class="info-2"><div class="info-item-1">Ollama CommandsOllama offers a variety of command-line tools (CLI) for interacting with locally running models. To see a list of available commands, you can use: 1ollama --help This will display the following: 12345678910111213141516171819202122232425Large language model runnerUsage:  ollama [flags]  ollama [command]Available Commands:  serve       Start ollama  create      Create a model from a Modelfile  show        Show information for a model  run         Run a model  stop        Stop a...</div></div></div></a><a class="pagination-related" href="/2025/01/29/ollama-intro/" title="Introduction to Ollama"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-01-29</div><div class="info-item-2">Introduction to Ollama</div></div><div class="info-2"><div class="info-item-1">Ollama is an open-source platform for large language models (LLMs), designed to make it easy for users to run, manage, and interact with LLMs directly on their local machines.   It provides a straightforward way to load and use various pre-trained language models, supporting a wide range of natural language processing tasks such as text generation, translation, code writing, and question answering.   What sets Ollama apart is its combination of ready-to-use models and tools with...</div></div></div></a><a class="pagination-related" href="/2025/01/29/ollama-install/" title="Installing Ollama"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-01-29</div><div class="info-item-2">Installing Ollama</div></div><div class="info-2"><div class="info-item-1">Ollama supports multiple operating systems, including macOS, Windows, Linux, and Docker containers.   It has modest hardware requirements, making it easy for users to run, manage, and interact with large language models locally.   Hardware and Software Requirements CPU: A multi-core processor (4 cores or more recommended).   GPU: If you plan to run large models or perform fine-tuning, a GPU with high computational power (e.g., NVIDIA with CUDA support) is recommended.   RAM: At least 8GB of...</div></div></div></a><a class="pagination-related" href="/2025/01/29/ollama-tutorial/" title="Ollama Tutorial"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-01-29</div><div class="info-item-2">Ollama Tutorial</div></div><div class="info-2"><div class="info-item-1"> Ollama is an open-source framework designed to make it easy to deploy and run large language models (LLMs) directly on your local machine. It supports multiple operating systems, including macOS, Windows, Linux, and even Docker containers. One of its standout features is model quantization, which significantly reduces GPU memory requirements, making it possible to run large models on everyday home computers. Who Is This Tutorial For?Ollama is ideal for developers, researchers, and users...</div></div></div></a><a class="pagination-related" href="/2025/01/29/ollama-run-model/" title="Running Models with Ollama"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-01-29</div><div class="info-item-2">Running Models with Ollama</div></div><div class="info-2"><div class="info-item-1">To run a model in Ollama, use the ollama run command.   For example, to run the DeepSeek-R1:8b model and interact with it, use the following command:  1ollama run deepseek-r1:8b If the model isn’t already installed, Ollama will automatically download it.   Once the download is complete, you can interact with the model directly in the terminal:   1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950C:\Users\Administrator&gt;ollama run...</div></div></div></a><a class="pagination-related" href="/2025/01/29/ollama-core-concepts/" title="Ollama Core Concepts"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-01-29</div><div class="info-item-2">Ollama Core Concepts</div></div><div class="info-2"><div class="info-item-1">Ollama is a localized machine learning framework designed for various natural language processing (NLP) tasks. It focuses on model loading, inference, and generation, making it easy for users to interact with large pre-trained models deployed locally. ModelsModels are the heart of Ollama. These are pre-trained machine learning models capable of performing tasks like text generation, summarization, sentiment analysis, and dialogue generation. Ollama supports a wide range of popular...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/image/IMG_3665.JPG" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Johnson Lin</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">486</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">62</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">25</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/iuiuu"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/iuiuu" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:me@linjiangxiong.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Installing-the-Python-SDK"><span class="toc-number">1.</span> <span class="toc-text">Installing the Python SDK</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Starting-the-Local-Service"><span class="toc-number">2.</span> <span class="toc-text">Starting the Local Service</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Using-the-Ollama-Python-SDK-for-Inference"><span class="toc-number">3.</span> <span class="toc-text">Using the Ollama Python SDK for Inference</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Custom-Clients"><span class="toc-number">4.</span> <span class="toc-text">Custom Clients</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Common-API-Methods"><span class="toc-number">5.</span> <span class="toc-text">Common API Methods</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Error-Handling"><span class="toc-number">6.</span> <span class="toc-text">Error Handling</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Posts</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/01/29/ollama-python-sdk/" title="Using Ollama with Python">Using Ollama with Python</a><time datetime="2025-01-29T07:17:04.000Z" title="Created 2025-01-29 15:17:04">2025-01-29</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/01/29/ollama-cli/" title="Interacting with Ollama Models">Interacting with Ollama Models</a><time datetime="2025-01-29T05:06:06.000Z" title="Created 2025-01-29 13:06:06">2025-01-29</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/01/29/ollama-core-concepts/" title="Ollama Core Concepts">Ollama Core Concepts</a><time datetime="2025-01-29T04:11:16.000Z" title="Created 2025-01-29 12:11:16">2025-01-29</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/01/29/ollama-commands/" title="Ollama Commands Overview">Ollama Commands Overview</a><time datetime="2025-01-29T03:16:03.000Z" title="Created 2025-01-29 11:16:03">2025-01-29</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/01/29/ollama-run-model/" title="Running Models with Ollama">Running Models with Ollama</a><time datetime="2025-01-29T02:11:15.000Z" title="Created 2025-01-29 10:11:15">2025-01-29</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2025 By Johnson Lin</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Reading Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light and Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle Between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Settings"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back to Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="https://cdn.jsdelivr.net/npm/hexo-theme-butterfly/source/js/utils.min.js"></script><script src="https://cdn.jsdelivr.net/npm/hexo-theme-butterfly/source/js/main.min.js"></script><div class="js-pjax"></div><script defer="defer" id="fluttering_ribbon" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-fluttering-ribbon.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>